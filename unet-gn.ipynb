{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import datasets, layers, models\n",
    "from keras.layers import Input, Dense, Conv2D, Flatten, Activation, concatenate, Dropout, Conv2DTranspose, LeakyReLU, Add\n",
    "from keras.layers import GlobalAveragePooling2D, Lambda, GlobalMaxPooling2D, MaxPooling2D\n",
    "from keras.layers import BatchNormalization as BN\n",
    "from keras.utils import Sequence\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os, shutil, glob\n",
    "import pydicom\n",
    "import cv2\n",
    "import math\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import albumentations as albu\n",
    "\n",
    "height = 512\n",
    "width = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image = np.load(\"/kaggle/input/project/data/p_train_image.npy\")\n",
    "train_label = np.load(\"/kaggle/input/project/data/p_train_label.npy\")\n",
    "valid_image = np.load(\"/kaggle/input/project/data/p_valid_image.npy\")\n",
    "valid_label = np.load(\"/kaggle/input/project/data/p_valid_label.npy\")\n",
    "test_image = np.load(\"/kaggle/input/project/data/p_test_image.npy\")\n",
    "test_label = np.load(\"/kaggle/input/project/data/p_test_label.npy\")\n",
    "print(\" train data: ({}, {}) \\n valid data: ({}, {})\\n test data: ({}, {})\".format(train_image.shape, train_label.shape, \n",
    "                                                                                valid_image.shape, valid_label.shape,\n",
    "                                                                                test_image.shape, test_label.shape,) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image = train_image.astype(np.float16)\n",
    "valid_image = valid_image.astype(np.float16)\n",
    "test_image = test_image.astype(np.float16)\n",
    "\n",
    "train_label = train_label.astype(np.bool)\n",
    "valid_label = valid_label.astype(np.bool)\n",
    "test_label = test_label.astype(np.bool)\n",
    "\n",
    "train_image = (train_image-2048)/2048\n",
    "valid_image = (valid_image-2048)/2048\n",
    "test_image = (test_image-2048)/2048\n",
    "print(train_image.shape, valid_image.shape, test_image.shape)\n",
    "print(\" train data: ({}, {}) \\n valid data: ({}, {})\\n test data: ({}, {})\".format(train_image.shape, train_label.shape, \n",
    "                                                                                valid_image.shape, valid_label.shape,\n",
    "                                                                                test_image.shape, test_label.shape,) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer, InputSpec\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras import backend as K\n",
    "\n",
    "class GroupNormalization(Layer):\n",
    "    def __init__(self,\n",
    "                 groups=16,\n",
    "                 axis=-1,\n",
    "                 epsilon=1e-5,\n",
    "                 center=True,\n",
    "                 scale=True,\n",
    "                 beta_initializer='zeros',\n",
    "                 gamma_initializer='ones',\n",
    "                 beta_regularizer=None,\n",
    "                 gamma_regularizer=None,\n",
    "                 beta_constraint=None,\n",
    "                 gamma_constraint=None,\n",
    "                 **kwargs):\n",
    "        super(GroupNormalization, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.groups = groups\n",
    "        self.axis = axis\n",
    "        self.epsilon = epsilon\n",
    "        self.center = center\n",
    "        self.scale = scale\n",
    "        self.beta_initializer = initializers.get(beta_initializer)\n",
    "        self.gamma_initializer = initializers.get(gamma_initializer)\n",
    "        self.beta_regularizer = regularizers.get(beta_regularizer)\n",
    "        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n",
    "        self.beta_constraint = constraints.get(beta_constraint)\n",
    "        self.gamma_constraint = constraints.get(gamma_constraint)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        dim = input_shape[self.axis]\n",
    "\n",
    "        if dim is None:\n",
    "            raise ValueError('Axis ' + str(self.axis) + ' of '\n",
    "                             'input tensor should have a defined dimension '\n",
    "                             'but the layer received an input with shape ' +\n",
    "                             str(input_shape) + '.')\n",
    "\n",
    "        if dim < self.groups:\n",
    "            raise ValueError('Number of groups (' + str(self.groups) + ') cannot be '\n",
    "                             'more than the number of channels (' +\n",
    "                             str(dim) + ').')\n",
    "\n",
    "        if dim % self.groups != 0:\n",
    "            raise ValueError('Number of groups (' + str(self.groups) + ') must be a '\n",
    "                             'multiple of the number of channels (' +\n",
    "                             str(dim) + ').')\n",
    "\n",
    "        self.input_spec = InputSpec(ndim=len(input_shape),\n",
    "                                    axes={self.axis: dim})\n",
    "        shape = (dim,)\n",
    "\n",
    "        if self.scale:\n",
    "            self.gamma = self.add_weight(shape=shape,\n",
    "                                         name='gamma',\n",
    "                                         initializer=self.gamma_initializer,\n",
    "                                         regularizer=self.gamma_regularizer,\n",
    "                                         constraint=self.gamma_constraint)\n",
    "        else:\n",
    "            self.gamma = None\n",
    "        if self.center:\n",
    "            self.beta = self.add_weight(shape=shape,\n",
    "                                        name='beta',\n",
    "                                        initializer=self.beta_initializer,\n",
    "                                        regularizer=self.beta_regularizer,\n",
    "                                        constraint=self.beta_constraint)\n",
    "        else:\n",
    "            self.beta = None\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        input_shape = K.int_shape(inputs)\n",
    "        tensor_input_shape = K.shape(inputs)\n",
    "\n",
    "        # Prepare broadcasting shape.\n",
    "        reduction_axes = list(range(len(input_shape)))\n",
    "        del reduction_axes[self.axis]\n",
    "        broadcast_shape = [1] * len(input_shape)\n",
    "        broadcast_shape[self.axis] = input_shape[self.axis] // self.groups\n",
    "        broadcast_shape.insert(1, self.groups)\n",
    "\n",
    "        reshape_group_shape = K.shape(inputs)\n",
    "        group_axes = [reshape_group_shape[i] for i in range(len(input_shape))]\n",
    "        group_axes[self.axis] = input_shape[self.axis] // self.groups\n",
    "        group_axes.insert(1, self.groups)\n",
    "\n",
    "        # reshape inputs to new group shape\n",
    "        group_shape = [group_axes[0], self.groups] + group_axes[2:]\n",
    "        group_shape = K.stack(group_shape)\n",
    "        inputs = K.reshape(inputs, group_shape)\n",
    "\n",
    "        group_reduction_axes = list(range(len(group_axes)))\n",
    "        \n",
    "        mean = K.mean(inputs, axis=group_reduction_axes, keepdims=True)\n",
    "        variance = K.var(inputs, axis=group_reduction_axes, keepdims=True)\n",
    "        \n",
    "        inputs = (inputs - mean) / (K.sqrt(variance + self.epsilon))\n",
    "\n",
    "        # prepare broadcast shape\n",
    "        inputs = K.reshape(inputs, group_shape)\n",
    "\n",
    "        outputs = inputs\n",
    "\n",
    "        # In this case we must explicitly broadcast all parameters.\n",
    "        if self.scale:\n",
    "            broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n",
    "            outputs = outputs * broadcast_gamma\n",
    "\n",
    "        if self.center:\n",
    "            broadcast_beta = K.reshape(self.beta, broadcast_shape)\n",
    "            outputs = outputs + broadcast_beta\n",
    "\n",
    "        # finally we reshape the output back to the input shape\n",
    "        outputs = K.reshape(outputs, tensor_input_shape)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'groups': self.groups,\n",
    "            'axis': self.axis,\n",
    "            'epsilon': self.epsilon,\n",
    "            'center': self.center,\n",
    "            'scale': self.scale,\n",
    "            'beta_initializer': initializers.serialize(self.beta_initializer),\n",
    "            'gamma_initializer': initializers.serialize(self.gamma_initializer),\n",
    "            'beta_regularizer': regularizers.serialize(self.beta_regularizer),\n",
    "            'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),\n",
    "            'beta_constraint': constraints.serialize(self.beta_constraint),\n",
    "            'gamma_constraint': constraints.serialize(self.gamma_constraint)\n",
    "        }\n",
    "        base_config = super(GroupNormalization, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_attention(n_channel, ratio):\n",
    "    shared_dense1 = Dense(n_channel // ratio)\n",
    "    shared_dense2 = Dense(n_channel)\n",
    "    def call(x):\n",
    "        if dim % self.groups != 0:\n",
    "            raise ValueError('Number of ratio (' + str(ratio) + ') must be a '\n",
    "            'multiple of the number of n_channel (' + str(n_channel) + ').')\n",
    "        height = x._keras_shape[1]\n",
    "        width = x._keras_shape[2]\n",
    "        \n",
    "        avg_pool = GlobalAveragePooling2D()(x)\n",
    "        avg_pool = shared_dense1(avg_pool)\n",
    "        avg_pool = LeakyReLU(alpha = 0.3)(avg_pool)\n",
    "        avg_pool = shared_dense2(avg_pool)\n",
    "        \n",
    "        max_pool = GlobalMaxPooling2D()(x)\n",
    "        max_pool = shared_dense1(max_pool)\n",
    "        max_pool = LeakyReLU(alpha = 0.3)(max_pool)\n",
    "        max_pool = shared_dense2(max_pool)\n",
    "        \n",
    "        merge_out = keras.layers.Add(avg_pool, max_pool)\n",
    "        merge_act = Activation('sigmoid')(merge_out)\n",
    "        merge_act = Lambda(lambda i: K.repeat_elements(i), height, axis=1)(merge_act)\n",
    "        merge_act = Lambda(lambda i: K.repeat_elements(i), width, axis=2)(merge_act)\n",
    "        \n",
    "        output = keras.layers.multiply([x, merge_act])\n",
    "        return output\n",
    "    return call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_attention(kernel_size):\n",
    "    def call(x):\n",
    "        avg_pool = Lambda(lambda i: K.mean(i, axis = 3, keepdims=False))(x)\n",
    "        max_pool = Lambda(lambda i: K.max(i, axis = 3, keepdims=False))(x)\n",
    "        concate = concatenate([avg_pool, max_pool], axis =-1)\n",
    "        con_out = Conv2D(filters=1, kernel_size=kernel_size, padding='same')\n",
    "        con_act = Activationvation('sigmoid')(con_out)\n",
    "        \n",
    "        output = keras.layers.multiply([x, con_act])\n",
    "        return output\n",
    "    return call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbam_block(ratio = 8):\n",
    "    def call(x):\n",
    "        n_channel = x._keras_shape[-1]\n",
    "        channel = channel_attention(n_channel, ratio)(x)\n",
    "        spatial = spatial_attention(kernel_size=7)(channel)\n",
    "        refined_feature = spatial\n",
    "    return refined_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convBlock(n_filter, kernel_size):\n",
    "    conv1 = Conv2D(filters=n_filter, kernel_size=kernel_size, padding='same')\n",
    "    conv2 = Conv2D(filters=n_filter, kernel_size=kernel_size, padding='same')\n",
    "    def call(x):\n",
    "        conv1_out = conv1(x)\n",
    "        conv1_gn = GroupNormalization()(conv1_out)\n",
    "        conv1_act = Activation('relu')(conv1_gn)\n",
    "        conv2_out = conv2(conv1_act)\n",
    "        conv2_gn = GroupNormalization()(conv2_out)\n",
    "        conv2_act = Activation('relu')(conv2_gn)\n",
    "        return conv2_act\n",
    "    return call\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResBlock(n_filter, kernel_size, with_conv_shortcut = False):\n",
    "    def call(x):\n",
    "        conv_out = convBlock(n_filter = n_filter, kernel_size = kernel_size)(x)\n",
    "        cbam_out = cbam_block(ratio = 16)(x)\n",
    "        \n",
    "        if with_conv_shortcutcut:\n",
    "            shortcut = Conv2D(filters=n_filter, kernel_size=kernel_size, padding='same')(x)\n",
    "            shortcut = GroupNormalization()(shortcut)\n",
    "            output =  keras.layers.Add(cbam_out, shortcut)\n",
    "        else:\n",
    "        output = keras.layers.Add(cbam_out, x)\n",
    "        \n",
    "        return output\n",
    "    return call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_gn(n_filter = 4, input_size = (512,512,1)):\n",
    "    inputs = Input(input_size)\n",
    "    # contracting path\n",
    "    conv1 = convBlock(n_filter=2**n_filter, kernel_size=(3,3))(inputs)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = convBlock(n_filter=2**(n_filter+1), kernel_size=(3,3))(pool1)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    conv3 = convBlock(n_filter=2**(n_filter+2), kernel_size=(3,3))(pool2)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    conv4 = convBlock(n_filter=2**(n_filter+3), kernel_size=(3,3))(pool3)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    \n",
    "    conv5 = convBlock(n_filter=2**(n_filter+3), kernel_size=(3,3))(pool4)\n",
    "    \n",
    "    # expansive path\n",
    "    u6 = Conv2DTranspose(filters=2**(n_filter+3), kernel_size=(2,2), strides=(2,2))(conv5)\n",
    "    concate6 = concatenate([conv4, u6], axis = 3)\n",
    "    conv6 = convBlock(n_filter=2**(n_filter+3), kernel_size=(3,3))(concate6)\n",
    "\n",
    "    u7 = Conv2DTranspose(filters=2**(n_filter+2), kernel_size=(2,2), strides=(2,2))(conv6)\n",
    "    concate7 = concatenate([conv3, u7], axis = 3)\n",
    "    conv7 = convBlock(n_filter=2**(n_filter+2), kernel_size=(3,3))(concate7)\n",
    "    \n",
    "    u8 = Conv2DTranspose(filters=2**(n_filter+1), kernel_size=(2,2), strides=(2,2))(conv7)\n",
    "    concate8 = concatenate([conv2, u8], axis =3)\n",
    "    conv8 = convBlock(n_filter=2**(n_filter+1), kernel_size=(3,3))(concate8)\n",
    "\n",
    "    \n",
    "    u9 = Conv2DTranspose(filters=2**(n_filter), kernel_size=(2,2), strides=(2,2))(conv8)\n",
    "    concate9 = concatenate([conv1, u9], axis =3)\n",
    "    conv9 = convBlock(n_filter=2**(n_filter), kernel_size=(3,3))(concate9)\n",
    "\n",
    "    # protonet\n",
    "    conv9_out = Conv2D(filters=6, kernel_size=(1,1), activation = 'sigmoid', padding = 'same', name='prototype')(conv9)\n",
    "    conv9_coe_out = Conv2D(filters=12, kernel_size=(3,3), padding='same')(conv9)\n",
    "    conv9_coe_out = LeakyReLU(alpha=0.4)(conv9_coe_out)\n",
    "    conv9_coe_out = Conv2D(filters=12, kernel_size=(3,3), padding='same')(conv9_coe_out)\n",
    "    conv9_coe_out = LeakyReLU(alpha=0.4)(conv9_coe_out)\n",
    "    conv9_coe_out = GlobalAveragePooling2D()(conv9_coe_out)\n",
    "    conv9_coe_out = Activation('tanh', name='reg_proto')(conv9_coe_out)\n",
    "    conv9_coe_out = Lambda(lambda x: K.reshape(x, shape=(-1, 2, 6)))(conv9_coe_out)\n",
    "    repeat_coe = Lambda(lambda x: K.repeat_elements(K.expand_dims(x, axis=1), height, axis=1))(conv9_coe_out)\n",
    "    repeat_coe = Lambda(lambda x: K.repeat_elements(K.expand_dims(x, axis=1), width, axis=1))(repeat_coe)\n",
    "    conv9_out = Lambda(lambda x: K.repeat_elements(K.expand_dims(x, axis=3), 2, axis=3))(conv9_out)\n",
    "    print(repeat_coe)\n",
    "    \n",
    "    assembly = Lambda(lambda x: K.sum(x[0] * x[1], axis=-1))([repeat_coe, conv9_out])\n",
    "    assembly = Lambda(lambda x: K.softmax(x, axis=-1), name='seg')(assembly)\n",
    "    print(assembly)\n",
    " \n",
    "    model = Model(inputs=[inputs], outputs=[assembly])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "height = 512\n",
    "width = 512\n",
    "\n",
    "def bce(y_true, y_pred):\n",
    "    y_sigmoid_pred = K.clip(y_pred, 1e-11, 0.99999999)\n",
    "    return -K.mean(y_true * K.log(y_sigmoid_pred) + (1-y_true) * K.log(1-y_sigmoid_pred), axis=-1)\n",
    "\n",
    "def dice_metric(y_true, y_pred):\n",
    "    dice = lambda x: 2 * x[0] / (x[1] + x[2] + 1e-6)\n",
    "    reshape = lambda x: K.reshape(x, (-1, height * width, 1))\n",
    "    div_tmp_AiB = K.sum(reshape(y_true[:,:,:,0] * y_pred[:,:,:,0]), axis=1)\n",
    "    div_tmp_A = K.sum(reshape(y_true[:,:,:,0]), axis=1)\n",
    "    div_tmp_B = K.sum(reshape(y_pred[:,:,:,0]), axis=1)\n",
    "    div_dice = dice([div_tmp_AiB, div_tmp_A, div_tmp_B])\n",
    "    indicate = K.max(reshape(y_true[:,:,:,0]), axis=1)\n",
    "    indicate_dice = K.sum(indicate * div_dice)\n",
    "    indicate_dice /= (K.sum(indicate)+1e-6)\n",
    "    return indicate_dice\n",
    "    \n",
    "def back_dice_metric(y_true, y_pred):\n",
    "    dice = lambda x: 2 * x[0] / (x[1] + x[2] + 1e-6)\n",
    "    reshape = lambda x: K.reshape(x, (-1, height * width, 1))\n",
    "    div_tmp_AiB = K.sum(reshape(y_true[:,:,:,1] * y_pred[:,:,:,1]), axis=1)\n",
    "    div_tmp_A = K.sum(reshape(y_true[:,:,:,1]), axis=1)\n",
    "    div_tmp_B = K.sum(reshape(y_pred[:,:,:,1]), axis=1)\n",
    "    div_dice = dice([div_tmp_AiB, div_tmp_A, div_tmp_B])\n",
    "    indicate = K.max(reshape(y_true[:,:,:,1]), axis=1)\n",
    "    indicate_dice = K.sum(indicate * div_dice)\n",
    "    indicate_dice /= (K.sum(indicate)+1e-6)\n",
    "    return indicate_dice\n",
    "\n",
    "def dice(y_true, y_pred):\n",
    "    dice = lambda x: 2 * x[0] / (x[1] + x[2] + 1e-6)\n",
    "    reshape = lambda x: K.reshape(x, (-1, height * width, 1))\n",
    "    div_tmp_AiB = K.sum(reshape(y_true[:,:,:,0] * y_pred[:,:,:,0]), axis=1)\n",
    "    div_tmp_A = K.sum(reshape(y_true[:,:,:,0]), axis=1)\n",
    "    div_tmp_B = K.sum(reshape(y_pred[:,:,:,0]), axis=1)\n",
    "    div_dice = dice([div_tmp_AiB, div_tmp_A, div_tmp_B])\n",
    "    indicate = K.max(reshape(y_true[:,:,:,0]), axis=1)\n",
    "    indicate_dice = K.sum(indicate * div_dice)\n",
    "    indicate_dice /= (K.sum(indicate)+1e-6)\n",
    "\n",
    "    return 1.0 - indicate_dice\n",
    "\n",
    "def ce_catogory(y_true, y_pred):\n",
    "    y_sigmoid_pred = K.clip(y_pred, 1e-11, 0.99999999)\n",
    "    return -K.mean(y_true * K.log(y_sigmoid_pred))\n",
    "\n",
    "def weight_tar_back_ce(y_true, y_pred):\n",
    "    reshape = lambda x: K.reshape(x, (-1, height * width, 1))\n",
    "    target_true = reshape(y_true[:,:,:,0])\n",
    "    target_pred = reshape(y_pred[:,:,:,0])\n",
    "    back_true = reshape(y_true[:,:,:,1])\n",
    "    back_pred = reshape(y_pred[:,:,:,1])\n",
    "    target_loss = target_true * K.log(target_pred) + (1-target_true) * K.log(1-target_pred)\n",
    "    back_loss = back_true * K.log(back_pred) + (1-back_true) * K.log(1-back_pred)\n",
    "    \n",
    "    loss = -(0.75*target_loss + 0.25*back_loss) \n",
    "#     return loss\n",
    "    return -target_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "current_time = time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime())\n",
    "\n",
    "num_epoch = 100\n",
    "batch = 8\n",
    "# model_fileName = str(current_time)+(\"_{}_{}_model.h5\".format(num_epoch, batch))\n",
    "self_callback = [\n",
    "    ModelCheckpoint(str(time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime()))+(\"_{}_{}_model.h5\".format(num_epoch, batch)), verbose=0, save_best_only=True, save_weights_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model = unet_gn(n_filter=5, input_size = (None, None,1))\n",
    "unet_model.compile(optimizer=Adam(lr=1e-4), loss={'seg': dice}, metrics={'seg': [dice_metric, back_dice_metric]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = unet_model.fit(\n",
    "    x=train_image, y=train_label,\n",
    "    batch_size=batch,\n",
    "    epochs=num_epoch,\n",
    "    callbacks=self_callback,\n",
    "    validation_data = (valid_image, valid_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def plot_history(histiry):\n",
    "#     plt.figure(figsize=(15,15))\n",
    "    # summarize history for accuracy\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(history.history['dice_metric'])\n",
    "    plt.plot(history.history['val_dice_metric'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    # summarize history for loss\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    current_time = time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime())\n",
    "    plt.savefig(current_time + \"_history.png\")\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_model():\n",
    "    model = unet(n_filter=4,input_size = (512, 512, 1))\n",
    "    return model\n",
    "def load_model(weight_file):\n",
    "    print(\"loading model\")\n",
    "    loaded_model = construct_model()\n",
    "    loaded_model.load_weights(weight_file)\n",
    "    print(\"loaded model\")\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "def plot_exp(dataset, datalabel, upper_bound=-1):\n",
    "    j=1\n",
    "    plt.figure(figsize=(20,20))\n",
    "    predict = unet_model.predict(dataset)\n",
    "#     predict = np.where(predict>0.9, 1, 0)\n",
    "    color1 = [\"gray\", \"m\" ]\n",
    "    color2 = [\"gray\", \"yellow\" ]\n",
    "    cmap1 = ListedColormap(color1)\n",
    "    cmap2 = ListedColormap(color2)\n",
    "    plt.subplot(5,5,1)\n",
    "    plt.imshow(np.zeros((512,512)), cmap='gray')\n",
    "    purple_patch = mpatches.Patch(color='m', alpha=0.6, label='y_true')\n",
    "    yellow_patch = mpatches.Patch(color='y', alpha=0.4, label='y_pred')\n",
    "    orange_patch = mpatches.Patch(color='orange', alpha=0.5, label='overlap')\n",
    "    plt.legend(handles=[purple_patch, yellow_patch, orange_patch])\n",
    "    for i in range(len(dataset[:upper_bound])):\n",
    "        plt.subplot(5,5,j+1)\n",
    "        plt.imshow(dataset[i].reshape(512, 512), cmap='gray')\n",
    "#         plt.imshow(datalabel[i].reshape(512, 512), cmap=cmap1, alpha=0.8)\n",
    "        plt.imshow(datalabel[i,:,:,0].reshape(512, 512), cmap=cmap1, alpha=0.8)\n",
    "#         plt.imshow(predict[i].reshape(512, 512), cmap=cmap2, alpha=0.6)\n",
    "        plt.imshow(predict[i,:,:,0].reshape(512, 512), cmap=cmap2, alpha=0.6)\n",
    "        j += 1\n",
    "        \n",
    "    current_time = time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime())\n",
    "    plt.savefig(current_time + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_exp(test_image, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(valid_image[3].reshape(512,512), cmap='gray')\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(valid_label[3].reshape(512,512), cmap='gray')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.imshow(valid_image[16].reshape(512, 512), cmap='gray')\n",
    "plt.subplot(2,2,4)\n",
    "plt.imshow(valid_label[16].reshape(512, 512), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
